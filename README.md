<div align="center">
# ğŸ¦… Saqr-MCP
</div>

<p align="center">
  <a href="https://www.python.org/downloads/">
    <img src="https://img.shields.io/badge/python-3.11%2B-blue" alt="Python Version">
  </a>
  <a href="LICENSE">
    <img src="https://img.shields.io/badge/license-MIT-green" alt="License">
  </a>
  <a href="https://github.com/astral-sh/uv">
    <img src="https://img.shields.io/badge/package%20manager-UV-orange" alt="UV Package Manager">
  </a>
</p>

Saqr-MCP is a powerful Python application that implements the Model Context Protocol (MCP) to enable advanced AI assistant capabilities. It supports both local models through Ollama and cloud models through Groq, providing a flexible client-server architecture. The server component offers a rich set of tools including web search, memory management, document generation, and advanced reasoning capabilities.

![image](https://github.com/user-attachments/assets/2ee374a5-8b63-4f5a-b7fd-5bdae3a05e37)

## âœ¨ Features

- ğŸ¤– Interactive chat interface for querying models
- ğŸ”„ Support for both local models (Ollama) and cloud models (Groq)
- ğŸ” Advanced web search capabilities using Tavily API
- ğŸ“ Word document generation from markdown content
- ğŸ§  Comprehensive memory management system using mem0
- ğŸ’­ Advanced reasoning and thought process tracking
- âš¡ Async architecture for efficient processing
- ğŸ¨ Visual loading animations for better user experience
- ğŸ“Š Session-based thought logging and analysis
- ğŸ“„ Document generation with markdown support

## ğŸ“‹ Prerequisites

- ğŸ Python 3.11 or higher
- ğŸ¦™ [Ollama](https://ollama.ai/) installed with local models (for local model usage)
- ğŸ“¦ [UV](https://github.com/astral-sh/uv) package manager (recommended)

## ğŸš€ Installation

1. Clone this repository:

   ```bash
   git clone https://github.com/ahmedhassan456/Saqr-MCP.git
   cd saqr-mcp
   ```

2. Create and activate a virtual environment (optional but recommended):

   ```bash
   uv venv
   # On Windows
   .venv\Scripts\activate
   # On Unix or MacOS
   source venv/bin/activate
   ```

3. Install dependencies:

   ```bash
   uv add -r requirements.txt
   ```

4. Set up environment variables:
   - Copy `.env.example` to `.env`
   - Configure the following variables:
     - ğŸ”‘ `MODEL_NAME`: Your preferred Ollama model (e.g., `qwen3:1.7b`)
     - ğŸ” `TAVILY_API_KEY`: Your Tavily API key from [Tavily website](https://app.tavily.com/home)
     - âš¡ `GROQ_MODEL_NAME`: Your preferred Groq model name
     - ğŸ” `GROQ_API_KEY`: Your Groq API key from [Groq website](https://console.groq.com/)
     - ğŸ§  `MEM0_API_KEY`: Your Mem0 API key from [Mem0 website](https://mem0.ai/)

## ğŸ’» Usage

1. For local model usage, ensure Ollama is running with your chosen model available

2. Configure the client:
   - By default, the application uses Ollama client (`from src.ollama_client import SaqrMCPClient`)
   - To use Groq instead, modify `main.py` to use `from src.groq_client import SaqrMCPClient`

3. Run the client:

   ```bash
   python main.py
   ```

4. Type your queries in the interactive console:

   ```
   MCP Client Started!
   Type your queries or 'quit' to exit.

   Query:
   ```

5. Type `quit` to exit the application

## ğŸ“ Project Structure

- ğŸ“„ `main.py` - Entry point that starts the MCP client
- ğŸ“‚ `src/`
  - ğŸ”„ `ollama_client.py` - MCP client implementation for Ollama models
  - âš¡ `groq_client.py` - MCP client implementation for Groq models
  - ğŸ› ï¸ `server.py` - MCP server implementation with all tools
  - ğŸ“ `logger.py` - Custom logging utilities with visual animations

## ğŸ› ï¸ Available Tools

The server implements a comprehensive set of tools for various functionalities:

### ğŸ” Web Search and Document Generation
- **web_search**: Performs real-time web searches using Tavily API to retrieve up-to-date information
- **word_file_generator**: Creates Microsoft Word documents from markdown content with proper formatting

### ğŸ§  Memory Management
- **add_memory**: Stores new memories with specified types and content in mem0
- **get_all_memories**: Retrieves all stored memories, optionally filtered by type
- **search_memories**: Performs semantic search through stored memories to find relevant information

### ğŸ’­ Reasoning and Thought Process
- **think**: Records thoughts and reasoning processes for complex problem-solving
- **get_thoughts**: Retrieves all thoughts recorded in the current session
- **clear_thoughts**: Clears all recorded thoughts from the current session
- **get_thought_stats**: Provides detailed statistics about recorded thoughts

## âš™ï¸ Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| ğŸ”‘ `OLLAMA_MODEL_NAME` | The name of the Ollama model to use (e.g., `qwen3:1.7b`) | None |
| ğŸ” `TAVILY_API_KEY` | Tavily API Key for web search capabilities | None |
| âš¡ `GROQ_MODEL_NAME` | The name of the Groq model to use | None |
| ğŸ” `GROQ_API_KEY` | Groq API Key for cloud model access | None |
| ğŸ§  `MEM0_API_KEY` | Mem0 API Key for memory management | None |

## ğŸ“¦ Dependencies

- ğŸ”„ `mcp[cli]` - Model Context Protocol implementation
- ğŸŒ `httpx` - HTTP client for Python
- ğŸ“ `loguru` - Python logging made simple
- âš¡ `groq` - Groq API client
- ğŸ¦™ `ollama` - Interface to Ollama for local models
- ğŸ” `tavily-python` - Search Engine tailored for AI agents
- ğŸš€ `fastapi` - Web framework for building APIs
- âš¡ `uvicorn` - ASGI server implementation
- ğŸ“„ `htmldocx` - HTML to DOCX converter
- ğŸ” `duckduckgo-search` - Search engine integration
- ğŸ“„ `python-docx` - DOCX file handling
- ğŸ“ `markdown` - Markdown processing
- ğŸ§  `mem0` - Memory management system

## ğŸ“„ License

[MIT](LICENSE)

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create your feature branch (`git checkout -b feature/amazing-feature`)
3. ğŸ’¾ Commit your changes (`git commit -m 'Add some amazing feature'`)
4. ğŸ“¤ Push to the branch (`git push origin feature/amazing-feature`)
5. ğŸ”„ Open a Pull Request
